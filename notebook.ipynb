{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Simulations - group project\n",
    "\n",
    "## Project 4 : Variance reduction in Monte Carlo integration via function approximation\n",
    "\n",
    "Francesca Bettinelli \\\n",
    "Marianna Dell'Otto \\\n",
    "Sophie Lequeu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-29T14:19:28.022212600Z",
     "start_time": "2023-12-29T14:19:25.755549800Z"
    }
   },
   "outputs": [],
   "source": [
    "from question2 import *\n",
    "from question4 import *\n",
    "from question5 import *\n",
    "from question4_new import *\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "See LaTeX report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "#### a) Crude Monte Carlo estimator\n",
    "We begin by estimating the integral *$I$* using a crude Monte Carlo estimator *$I_{MC}$*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exact value of the integral (reference)\n",
    "ref_value = (1/5) * np.arctan(5)\n",
    "\n",
    "# crude Monte Carlo estimators\n",
    "N = 100\n",
    "nb_samples = np.logspace(np.log10(10), np.log10(10000), num=N, dtype=int) # evenly spaced values on a logarithmic scale\n",
    "\n",
    "CMC_estims = np.zeros(N)\n",
    "for M in range(N):\n",
    "    unif_samps = np.random.uniform(0, 1, nb_samples[M])\n",
    "    CMC_estims[M] = crude_MC(unif_samps, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot log-log graph to see order of the error\n",
    "loglog_graph(nb_samples, CMC_estims, ref_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the error of the crude Monte Carlo estimator for different values of M (in log-log scale), we observe that it can be approximated by a straight line with slope $\\approx 0.5$. We conclude that the error decreases approximately as the square-root of the number of samples M. This is indeed what was expected from a crude Monte Carlo estimator, since it is unbiaised, with variance in $\\mathcal{O}(M)$, thus confidence interval in $\\mathcal{O}(\\sqrt M)$ (from the CLT)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) MCLS\n",
    "The aim of this second part is to implement the MCLS estimators *$I_{MCLS}$* using an expansion of the Legendre polynomials up to degree n.\n",
    "We will consider the same values of $M$ (numbers of samples) as in *(a)*, and try different values of $n$ : \\\n",
    "    $n = 10, 20, 30, 50$, \\\n",
    "    $n = \\lceil \\sqrt M \\rceil$, \\\n",
    "    $n = \\lceil \\frac{M}{2} \\rceil$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_value = (1/5) * np.arctan(5)\n",
    "N = 100\n",
    "nb_samples = np.logspace(np.log10(10), np.log10(10000), num=N, dtype=int) # evenly spaced values on a logarithmic scale\n",
    "\n",
    "trials_n = [10, 20, 30, 50, -1, -2]\n",
    "nb_trials_n = len(trials_n)\n",
    "\n",
    "# MCLS estimators\n",
    "MCLS_estims = [-1 * np.ones(N) for _ in range(nb_trials_n)]\n",
    "MCLS_prime_estims = [-1 * np.ones(N) for _ in range(nb_trials_n)]\n",
    "\n",
    "# condition number of the Vandermonde matrix\n",
    "MCLS_cond = [-1 * np.ones(N) for _ in range(nb_trials_n)]\n",
    "MCLS_prime_cond = [-1 * np.ones(N) for _ in range(nb_trials_n)]\n",
    "\n",
    "for M in range(N):\n",
    "    for i in range(nb_trials_n):\n",
    "        n = trials_n[i]\n",
    "\n",
    "        if n == -1:\n",
    "            n = np.ceil(np.sqrt(nb_samples[M])).astype(int)\n",
    "            if nb_samples[M] > 5000: # too computationally expensive\n",
    "                break\n",
    "        if n == -2:\n",
    "            n = np.ceil((nb_samples[M])/2).astype(int)\n",
    "            if nb_samples[M] > 5000: # too computationally expensive\n",
    "                break\n",
    "\n",
    "        if n < nb_samples[M]:\n",
    "            unif_samps = np.random.uniform(0, 1, nb_samples[M])\n",
    "            MCLS_estims[i][M], MCLS_cond[i][M] = MCLS_new(unif_samps, f, n)\n",
    "            MCLS_prime_estims[i][M], MCLS_prime_cond[i][M] = MCLS_prime_new(unif_samps, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot log-log graph to see order of the error\n",
    "multiple_loglog_graph(nb_samples, MCLS_estims, ref_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot, we can see that increasing the polynomial degree $n$ (with $n$ fixed w.r.t. the number of samples $M$) reduces the convergence constant. If we set $n = \\lceil \\sqrt M \\rceil$, the order of convergence significantly improves. Instead, if we set $n = \\lceil \\frac{M}{2} \\rceil$, after an initial phase affected by noise, the order seems to stabilize, again, around $\\mathcal{O}(1 / \\sqrt{M})$.\n",
    "FROM THE REGRESSION LINES, IT SEEMS THAT INCREASING $n$ ALSO IMPROVES THE ORDER OF CONVERGENCE, MAYBE WE SHOULD AVOID DOING A REGRESSION IN THE FIRST PART OF THE PLOT WHERE THERE IS TOO MUCH NOISE???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot log-log graph to see order of the error\n",
    "multiple_loglog_graph(nb_samples, MCLS_prime_estims, ref_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot, we can deduce that $I_{MCLS}'$ does not provide benefits in terms of order of convergence w.r.t. crude MC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_cond_loglog_graph(nb_samples, MCLS_cond)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we choose a value of $n$ too small w.r.t. $M$, the least squares problems becomes very ill-conditioned. If we set $n = \\lceil \\sqrt M \\rceil$, the condition number of the Vandermonde matrix seems to be kept under control. In all these cases, the condition number tends to 1 as $M \\to \\infty$. Instead, if we set $n = \\lceil \\frac{M}{2} \\rceil$, the condition number explodes as $M$ increases; this might be the reason behind the bad order of convergence of this estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "In this question, we propose an expression for $I_{MCLS}$ and $I_{MCLS}'$ generalized to the case of importance sampling. Samples $x^{(i)}$'s are now drawn from a distribution defined by the probability density function $g(x) = \\frac{1}{w(x)}$.\n",
    "We propose : \n",
    "$$I_{IS-MCLS} = \\frac{\\sum_{i=1}^M \\left(f(x^{(i)}) - \\sum_{j=0}^n c_j^{**} \\phi_j(x^{(i)})\\right) w(x^{(i)})}{\\sum_{i=1}^M w(x^{(i)})} + c_0^{**}$$ \n",
    "and \n",
    "$$I_{IS-MCLS}' = c_0^{**}$$ \n",
    "\n",
    "where $\\textbf{c**} = \\argmin_{\\textbf{c} \\in \\mathbb{R}^{n+1}} \\sum_{i=1}^M w(x^{(i)}) \\left(f(x^{(i)}) - \\sum_{j=0}^n c_j \\phi_j (x^{(i)}) \\right)^2$\n",
    "\n",
    "as generalized estimators IS-MCLS and IS-MCLS'. \\\n",
    "In the LaTeX report, we verify that these verify the properties stated in Question 1 for classical $I_{IS-MCLS}$ and $I_{IS-MCLS}'$ estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "- Propose a method to sample from the density $h(x) := \\frac{1}{w(x)}$ : \\\n",
    "We will be using the Acceptance-rejection method. This method is the one proposed in ref [2], page 6, where we only consider the case where function f has values in $R$ (and not $R^d$). We see 2 possible variations : \n",
    "\n",
    "    $\\diamond$ Method 1 : \\\n",
    "We need a bound on the true propability density function. Knowing that every $\\phi_j^2$ can be bounded by $4 e p_1^{\\infty}(x)$, $h(x)$ can be bounded by the same quantity. We denote $g(x) = p_1^{\\infty}(x) = \\frac{1}{\\pi \\sqrt{x(1-x)}}$ (our bound, without the multiplicative constant).\n",
    "    1.  Then we sample $Y$ from this bound $g(x)$. Since this bound has a nice expression, this can be done by Inverse transform method.\n",
    "    We can easily check that the cdf associated to pdf $g(x)$ is : $G(x) = \\frac{2}{\\pi} \\arcsin \\sqrt{x}$, and the inverse function is : $G^{-1}(x) = \\sin^2(\\frac{\\pi y}{2})$.\n",
    "    2.  We sample $U$ from uniform on $[0, 1]$, independant of $Y$.\n",
    "    3.  If $U \\leq \\frac{h(Y)}{Cg(Y)} $, set $X=Y$, otherwise return to step 2.\n",
    "\n",
    "- Implement the MCLS estimators with importance sampling.\n",
    "- Test on same number of samples as in Question 2. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1. Test sampling from bound g, based on inverse transform method\n",
    "Y_samples = sample_from_g(1000)\n",
    "visualize_cdf_from_samples(Y_samples)\n",
    "\n",
    "# Looks good, similar to expected distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Expected pdf and cdf :\n",
    "\n",
    "<div style=\"display: flex; flex-direction: row;\">\n",
    "    <img src=\"Arcsin_density.png\" alt=\"Image 1\" style=\"width: 50%; height: auto; margin-right: 10px;\">\n",
    "    <img src=\"Arcsin_cdf.png\" alt=\"Image 2\" style=\"width: 50%; height: auto;\">\n",
    "</div>\n",
    "\n",
    "Source : Wikipedia, Arcsine distribution, https://en.wikipedia.org/wiki/Arcsine_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Test sampling from h = 1/w, done by acceptance-rejection method\n",
    "H_samples = sample_from_h(500, 30000, 10)\n",
    "visualize_cdf_from_samples(H_samples)\n",
    "\n",
    "# Acceptance rate is low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_bound_g_on_h(10)\n",
    "\n",
    "# bound seems way too loose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 2: composition method for a mixture density + AR method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_samples = sample_from_h_new(500, 30000, 10)\n",
    "visualize_cdf_from_samples(H_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C = 4 * np.e\n",
    "n = 3\n",
    "C = 1 / (n + 1/4) # can a similar constant be found analytically somehow?\n",
    "x = np.linspace(0.01, 0.99, 100)\n",
    "plt.plot(x, eval_phi_squared(x, n))\n",
    "plt.plot(x, C * g(x))\n",
    "print(np.all(eval_phi_squared(x, n) < C * g(x)))\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Question 5\n",
    "- Implementation of the above methods to the FitzHugh-Nagumo system of ODEs.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "epsilon = 0.08\n",
    "I = 1.0\n",
    "v0 = 0\n",
    "w0 = 0\n",
    "t0 = 0\n",
    "T = 10\n",
    "Nt = 1000\n",
    "# a and b are uniformly distributed random parameters\n",
    "a = np.random.uniform(0.6, 0.8)\n",
    "b = np.random.uniform(0.7, 0.9)\n",
    "\n",
    "v, w, t = solve_FHN(epsilon, I, v0, w0, t0, T, Nt, a, b)\n",
    "v_scipy, w_scipy, t_scipy = solve_FHN_scipy(epsilon, I, v0, w0, t0, T, Nt, a, b)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T14:19:47.469138200Z",
     "start_time": "2023-12-29T14:19:47.391203900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(t, v)\n",
    "plt.plot(t, w)\n",
    "plt.plot(t_scipy, v_scipy)\n",
    "plt.plot(t_scipy, w_scipy)\n",
    "plt.legend(['v', 'w', 'v_scipy', 'w_scipy'])\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot the phase space\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot the integral curves \n",
    "plt.plot(v, w, 'k-', label='Integral Curves')\n",
    "\n",
    "# Plot the cubic nullcline \n",
    "v_values = np.linspace(-2, 2, 100)\n",
    "w_values_cubic = v_values - v_values**3 / 3 + I\n",
    "plt.plot(v_values, w_values_cubic, 'g-', label='Cubic Nullcline')\n",
    "\n",
    "# Plot the linear nullcline \n",
    "w_values_linear = (v_values + a) / b\n",
    "plt.plot(v_values, w_values_linear, 'r-', label='Linear Nullcline')\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('v')\n",
    "plt.ylabel('w')\n",
    "plt.title('FitzHugh-Nagumo Model Phase Space')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Question 5.b"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "N = 100\n",
    "nb_samples = np.logspace(np.log10(10), np.log10(10000), num=N, dtype=int) # evenly spaced values on a logarithmic scale\n",
    "          \n",
    "n=0  #n=0,1,2,3     \n",
    "a_unif = np.random.uniform(0.6, 0.8, N)\n",
    "b_unif = np.random.uniform(0.7, 0.9, N)\n",
    "#MCLS_estims[i][M] = MCLS_multiple(a_unif, b_unif, n, epsilon, I, v0, w0, t0, T, Nt)\n",
    "MCLS_estims = MCLS_multiple(a_unif, b_unif, n, epsilon, I, v0, w0, t0, T, Nt)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T14:31:41.312029600Z",
     "start_time": "2023-12-29T14:28:45.218076Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.38716656877463\n"
     ]
    }
   ],
   "source": [
    "print(MCLS_estims)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T14:47:39.817706500Z",
     "start_time": "2023-12-29T14:47:39.806157700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
